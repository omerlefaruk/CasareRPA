# CasareRPA Complete Architecture Roadmap
**Project Aether: Enterprise-Grade Distributed RPA Platform**

**Status**: Phases 1-6 Complete, Phase 7+ In Progress
**Goal**: Production-ready distributed RPA with durable execution, multi-robot coordination, and enterprise security
**Last Updated**: 2025-11-30

---

## Executive Summary

This roadmap details the transformation of CasareRPA into a **production-ready enterprise RPA platform** with:

- âœ… **Durable Execution** (DBOS Transact) - automatic crash recovery, exactly-once semantics
- âœ… **Distributed Queue** (PgQueuer) - 18k+ jobs/sec throughput, LISTEN/NOTIFY efficiency
- âœ… **Orchestrator Integration** - dual backend (in-memory dev, PgQueuer prod)
- âœ… **Self-Healing Selectors** (Tier 1) - heuristic-based UI adaptation
- âœ… **Enterprise Security** - SQL injection prevention, workflow validation, secure logging
- âœ… **Monitoring Dashboard** - FastAPI + React real-time dashboard
- âœ… **Robot Agent** - Distributed execution with Playwright auto-install
- ğŸš§ **Multi-Robot Coordination** - capability matching, failover, state affinity
- ğŸš§ **Resource Pooling** - unified browser/database/HTTP connection management
- ğŸš§ **Cloud Deployment** - DBOS Cloud managed hosting

---

## Technology Stack

### Core Infrastructure
| Component | Technology | Purpose | Status |
|-----------|-----------|---------|--------|
| Durable Execution | DBOS Transact | Workflow orchestration with automatic checkpointing | âœ… Complete |
| Distributed Queue | PgQueuer | Job distribution with 18k+ jobs/sec throughput | âœ… Complete |
| Database | PostgreSQL 15+ | Persistence, queue backend, resource locks | âœ… Complete |
| API Framework | FastAPI | REST API with async support, auto OpenAPI docs | âœ… Complete |
| Monitoring UI | React + Vite | Real-time dashboard with TanStack Query | âœ… Complete |
| Security | Input Validators | SQL injection, workflow validation, secure logging | âœ… Complete |
| Backend Platform | Supabase | Full-stack backend (DB, Realtime, Vault) | ğŸš§ Partial |
| Secrets Management | HashiCorp Vault | Encrypted credential storage | ğŸš§ Planned |
| Cloud Deployment | DBOS Cloud | Managed hosting with auto-scaling | ğŸš§ Planned |

### Automation Capabilities
| Component | Technology | Purpose | Status |
|-----------|-----------|---------|--------|
| Web Automation | Playwright | Browser control with auto-wait, network interception | âœ… Complete |
| Desktop Automation | UIAutomation | Windows desktop element interaction | âœ… Complete |
| Self-Healing | Multi-tier fallback | Heuristic â†’ Anchor â†’ Computer Vision | ğŸš§ Tier 1 only |
| Resource Pooling | Custom UnifiedResourceManager | Connection pooling for browser/DB/HTTP | ğŸš§ Planned |

---

## Current Architecture State

### âœ… Completed Components (Phase 1-3.3)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORCHESTRATOR âœ…                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OrchestratorEngine                                   â”‚  â”‚
â”‚  â”‚    â”œâ”€ QueueAdapter (dual backend) âœ…                  â”‚  â”‚
â”‚  â”‚    â”‚   â”œâ”€ In-memory mode (development)               â”‚  â”‚
â”‚  â”‚    â”‚   â””â”€ PgQueuer mode (production)                 â”‚  â”‚
â”‚  â”‚    â”œâ”€ JobScheduler (APScheduler)                     â”‚  â”‚
â”‚  â”‚    â””â”€ TriggerManager (manual, scheduled, webhook)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               WORKFLOW EXECUTION (DBOS) âœ…                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  @DBOS.workflow()                                     â”‚  â”‚
â”‚  â”‚    â”œâ”€ initialize_context_step() [@step]             â”‚  â”‚
â”‚  â”‚    â”œâ”€ execute_node_step() Ã— N [@step]               â”‚  â”‚
â”‚  â”‚    â””â”€ cleanup_context_step() [@step]                â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Features:                                            â”‚  â”‚
â”‚  â”‚    âœ… Automatic checkpointing                         â”‚  â”‚
â”‚  â”‚    âœ… Crash recovery                                  â”‚  â”‚
â”‚  â”‚    âœ… Exactly-once execution                          â”‚  â”‚
â”‚  â”‚    âœ… workflow_id idempotency                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DISTRIBUTED QUEUE (PgQueuer) âœ…                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PgQueuerProducer (Orchestrator)                      â”‚  â”‚
â”‚  â”‚    - 18k+ jobs/sec throughput                        â”‚  â”‚
â”‚  â”‚    - Priority levels (0-20)                          â”‚  â”‚
â”‚  â”‚    - Multi-tenancy (RLS)                              â”‚  â”‚
â”‚  â”‚  PgQueuerConsumer (Robots - Phase 3.4)               â”‚  â”‚
â”‚  â”‚    - Job claiming (FOR UPDATE SKIP LOCKED)            â”‚  â”‚
â”‚  â”‚    - Visibility timeout + heartbeat                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SELF-HEALING SELECTORS (Tier 1) âœ…                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  HeuristicSelectorHealer                              â”‚  â”‚
â”‚  â”‚    â”œâ”€ Multi-attribute fallback (400ms budget)        â”‚  â”‚
â”‚  â”‚    â”œâ”€ Fragility scoring                              â”‚  â”‚
â”‚  â”‚    â””â”€ Telemetry collection                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš§ Components Under Development

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ROBOT AGENT ARCHITECTURE ğŸš§                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RobotAgent (runs on worker machines)                 â”‚  â”‚
â”‚  â”‚    â”œâ”€ Registration & capability advertisement        â”‚  â”‚
â”‚  â”‚    â”œâ”€ Hybrid poll + subscribe (PgQueuer + Realtime)  â”‚  â”‚
â”‚  â”‚    â”œâ”€ Job claiming with constraint matching          â”‚  â”‚
â”‚  â”‚    â”œâ”€ Workflow execution (DBOS durable)              â”‚  â”‚
â”‚  â”‚    â”œâ”€ Heartbeat protocol (lease extension)           â”‚  â”‚
â”‚  â”‚    â”œâ”€ Resource pooling (browser/DB/HTTP)             â”‚  â”‚
â”‚  â”‚    â”œâ”€ Graceful shutdown & failover                   â”‚  â”‚
â”‚  â”‚    â””â”€ State affinity management                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           UNIFIED RESOURCE MANAGER ğŸš§                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  UnifiedResourceManager                               â”‚  â”‚
â”‚  â”‚    â”œâ”€ BrowserPool (Playwright contexts)              â”‚  â”‚
â”‚  â”‚    â”œâ”€ DatabasePool (AsyncPG/AsyncMySQL)              â”‚  â”‚
â”‚  â”‚    â”œâ”€ HTTPPool (HTTPX sessions)                      â”‚  â”‚
â”‚  â”‚    â”œâ”€ Job-level quota enforcement                    â”‚  â”‚
â”‚  â”‚    â”œâ”€ Resource lease tracking                        â”‚  â”‚
â”‚  â”‚    â””â”€ Cleanup strategies (TTL, LRU, explicit)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          EVENT-DRIVEN RUNTIME (Supabase) ğŸš§                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Supabase Realtime Channels                           â”‚  â”‚
â”‚  â”‚    â”œâ”€ Postgres Changes (CDC for job inserts)         â”‚  â”‚
â”‚  â”‚    â”œâ”€ Broadcast (control commands, cancellations)    â”‚  â”‚
â”‚  â”‚    â”œâ”€ Presence (robot health, load balancing)        â”‚  â”‚
â”‚  â”‚    â””â”€ Hybrid poll+subscribe for resilience           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SECURITY & SECRETS ğŸš§                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Supabase Vault / HashiCorp Vault                    â”‚  â”‚
â”‚  â”‚    - Credential storage (encrypted at rest)          â”‚  â”‚
â”‚  â”‚    - Dynamic secrets generation                       â”‚  â”‚
â”‚  â”‚    - Audit logging for compliance                    â”‚  â”‚
â”‚  â”‚    - Lease-based credential lifecycle                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MONITORING & OBSERVABILITY ğŸš§                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OpenTelemetry                                        â”‚  â”‚
â”‚  â”‚    - Distributed tracing (workflow â†’ nodes)          â”‚  â”‚
â”‚  â”‚    - Metrics (job duration, queue depth, robot CPU)  â”‚  â”‚
â”‚  â”‚    - Logs aggregation (structured JSON)             â”‚  â”‚
â”‚  â”‚    - Backend: Honeycomb/DataDog/Grafana Cloud        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Roadmap

### Phase 1: Foundation âœ… COMPLETE
**Goal**: Establish durable execution with DBOS Transact
**Status**: Complete (Commit: `441a9bb`)

**Delivered**:
- DBOS Transact integration with @workflow and @step decorators
- ExecutionLifecycleManager for centralized state management
- Automatic checkpointing and crash recovery
- Exactly-once execution semantics
- workflow_id-based idempotency

**Code Example**:
```python
from dbos import DBOS

@DBOS.workflow()
async def run_workflow_durable(workflow: WorkflowDefinition, workflow_id: str):
    """Durable workflow execution with automatic recovery."""

    # Step 1: Initialize context
    context = await initialize_context_step(workflow.initial_variables)

    # Step 2: Execute nodes (checkpoint after each)
    for node in workflow.nodes:
        result = await execute_node_step(node, context)
        context.update(result)

    # Step 3: Cleanup
    await cleanup_context_step(context)

    return {"success": True, "variables": context.variables}

@DBOS.step()
async def execute_node_step(node: Node, context: ExecutionContext):
    """Single node execution - checkpointed automatically."""
    return await node.execute(context)
```

### Phase 2: Distributed Queue âœ… COMPLETE
**Goal**: Implement PgQueuer for high-throughput job distribution
**Status**: Complete (Commit: `5607fba`)

**Delivered**:
- PgQueuer integration (18k+ jobs/sec throughput)
- PostgreSQL-based queue with LISTEN/NOTIFY
- Priority-based job scheduling (0-20 levels)
- Multi-tenancy with Row-Level Security (RLS)
- Dead Letter Queue (DLQ) for failed jobs

**Architecture**:
```sql
-- PgQueuer job table schema
CREATE TABLE pgqueuer_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    queue_name TEXT NOT NULL,
    priority INTEGER DEFAULT 10,
    payload JSONB NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('pending', 'claimed', 'completed', 'failed')),
    claimed_at TIMESTAMPTZ,
    claimed_by TEXT,
    visibility_timeout TIMESTAMPTZ,
    max_retries INTEGER DEFAULT 3,
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ
);

-- Index for fast job claiming
CREATE INDEX idx_jobs_claimable ON pgqueuer_jobs (queue_name, priority DESC, created_at)
WHERE status = 'pending' AND (visibility_timeout IS NULL OR visibility_timeout < NOW());

-- Notify trigger for instant job notifications
CREATE OR REPLACE FUNCTION notify_job_insert()
RETURNS TRIGGER AS $$
BEGIN
    PERFORM pg_notify('pgqueuer_jobs', NEW.queue_name);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER job_insert_notify
AFTER INSERT ON pgqueuer_jobs
FOR EACH ROW EXECUTE FUNCTION notify_job_insert();
```

**Job Claiming** (FOR UPDATE SKIP LOCKED):
```python
async def claim_job(self, queue_name: str, robot_id: str) -> Optional[Job]:
    """Atomically claim a job from the queue."""
    query = """
        UPDATE pgqueuer_jobs
        SET status = 'claimed',
            claimed_at = NOW(),
            claimed_by = $2,
            visibility_timeout = NOW() + INTERVAL '30 seconds'
        WHERE id = (
            SELECT id FROM pgqueuer_jobs
            WHERE queue_name = $1
              AND status = 'pending'
              AND (visibility_timeout IS NULL OR visibility_timeout < NOW())
            ORDER BY priority DESC, created_at ASC
            LIMIT 1
            FOR UPDATE SKIP LOCKED
        )
        RETURNING *;
    """
    row = await self.pool.fetchrow(query, queue_name, robot_id)
    return Job.from_row(row) if row else None
```

### Phase 3.3: Orchestrator Integration âœ… COMPLETE
**Goal**: Integrate PgQueuer into OrchestratorEngine with dual backend support
**Status**: Complete (Commit: `5607fba`)

**Delivered**:
- OrchestratorConfig with `use_pgqueuer` flag
- QueueAdapter for backend abstraction
- Automatic fallback when PgQueuer unavailable
- Full backward compatibility
- 15/15 tests passing

**Configuration**:
```python
# Development mode
config = OrchestratorConfig(use_pgqueuer=False)
engine = OrchestratorEngine(config=config)

# Production mode
config = OrchestratorConfig(
    use_pgqueuer=True,
    postgres_url="postgresql://user:pass@host:5432/casare_rpa",
)
engine = OrchestratorEngine(config=config)
```

---

### Phase 3.4: Robot Agent Implementation ğŸš§ IN PROGRESS
**Status**: CLI Complete, Agent Core In Progress
**Priority**: ğŸ”´ CRITICAL
**Dependencies**: Phase 3.3 âœ…

#### Why This is Critical

With Phase 3.3 complete, jobs can be enqueued to PgQueuer, but **no workers exist to claim and execute them**. Phase 3.4 implements the Robot Agent that:

1. **Registers** with orchestrator, advertising capabilities
2. **Claims jobs** from PgQueuer using FOR UPDATE SKIP LOCKED
3. **Executes workflows** with DBOS durability
4. **Sends heartbeats** to extend job lease
5. **Reports status** via heartbeat protocol
6. **Handles graceful shutdown** without job loss
7. **Manages resources** via UnifiedResourceManager
8. **Coordinates** via Supabase Realtime channels

#### Architecture

```python
# src/casare_rpa/robot/distributed_agent.py

@dataclass
class RobotCapabilities:
    """Robot capability advertisement."""
    platform: str  # "windows", "linux", "macos"
    browser_engines: List[str]  # ["chromium", "firefox", "webkit"]
    desktop_automation: bool
    max_concurrent_jobs: int
    cpu_cores: int
    memory_gb: float
    tags: List[str]  # ["gpu", "high-memory", "sap-certified"]

@dataclass
class RobotRegistration:
    """Robot registration payload."""
    robot_id: str
    hostname: str
    capabilities: RobotCapabilities
    heartbeat_interval_seconds: int = 10
    registered_at: str = field(default_factory=lambda: datetime.now().isoformat())

class DistributedRobotAgent:
    """
    Distributed Robot agent with multi-robot coordination.

    Responsibilities:
    - Register with orchestrator
    - Hybrid poll + subscribe for jobs
    - Execute workflows with DBOS
    - Resource pooling (browser/DB/HTTP)
    - Heartbeat protocol
    - Graceful shutdown
    - State affinity management
    """

    def __init__(self, config: RobotConfig):
        self.config = config
        self.robot_id = config.robot_id or str(uuid.uuid4())

        # PgQueuer consumer
        self.consumer = PgQueuerConsumer(
            postgres_url=config.postgres_url,
            robot_id=self.robot_id,
            visibility_timeout_seconds=30
        )

        # Supabase Realtime client
        self.realtime_client = RealtimeClient(
            url=config.supabase_url,
            api_key=config.supabase_key
        )

        # Unified resource manager
        self.resource_manager = UnifiedResourceManager(
            browser_pool_size=config.browser_pool_size,
            db_pool_size=config.db_pool_size,
            http_pool_size=config.http_pool_size
        )

        # DBOS workflow executor
        self.executor = DBOSWorkflowExecutor()

        # Running state
        self._running = False
        self._current_job: Optional[Job] = None

    async def start(self):
        """Start robot agent with registration."""
        logger.info(f"Starting Robot {self.robot_id}")

        # Register with orchestrator
        await self._register()

        # Start resource pools
        await self.resource_manager.start()

        # Start PgQueuer consumer
        await self.consumer.start()

        # Connect to Realtime channels
        await self._setup_realtime_channels()

        self._running = True

        # Start concurrent tasks
        await asyncio.gather(
            self._hybrid_job_loop(),
            self._heartbeat_loop(),
            self._presence_loop()
        )

    async def _register(self):
        """Register robot with orchestrator."""
        capabilities = RobotCapabilities(
            platform=sys.platform,
            browser_engines=["chromium", "firefox", "webkit"],
            desktop_automation=sys.platform == "win32",
            max_concurrent_jobs=self.config.max_concurrent_jobs,
            cpu_cores=os.cpu_count() or 1,
            memory_gb=psutil.virtual_memory().total / (1024**3),
            tags=self.config.tags
        )

        registration = RobotRegistration(
            robot_id=self.robot_id,
            hostname=socket.gethostname(),
            capabilities=capabilities
        )

        # Insert into robots table
        await self.consumer.pool.execute("""
            INSERT INTO robots (robot_id, hostname, capabilities, status, registered_at)
            VALUES ($1, $2, $3, 'idle', NOW())
            ON CONFLICT (robot_id) DO UPDATE
            SET capabilities = $3, status = 'idle', registered_at = NOW()
        """, self.robot_id, socket.gethostname(), orjson.dumps(asdict(registration)))

        logger.info(f"Robot registered: {self.robot_id}")

    async def _setup_realtime_channels(self):
        """Setup Supabase Realtime channels."""

        # Channel 1: Postgres Changes (CDC for job inserts)
        jobs_channel = self.realtime_client.channel("jobs")
        jobs_channel.on("postgres_changes", {
            "event": "INSERT",
            "schema": "public",
            "table": "pgqueuer_jobs",
            "filter": f"queue_name=eq.default"
        }, self._on_job_inserted)

        # Channel 2: Broadcast (control commands)
        control_channel = self.realtime_client.channel("control")
        control_channel.on("broadcast", {"event": "cancel_job"}, self._on_cancel_job)
        control_channel.on("broadcast", {"event": "shutdown"}, self._on_shutdown)

        # Channel 3: Presence (robot health)
        presence_channel = self.realtime_client.channel("robots")
        presence_channel.on("presence", {"event": "sync"}, self._on_presence_sync)

        await jobs_channel.subscribe()
        await control_channel.subscribe()
        await presence_channel.subscribe()

    async def _hybrid_job_loop(self):
        """Hybrid poll + subscribe job loop."""
        subscribe_timeout = 5.0  # Wait 5s for Realtime notification

        while self._running:
            try:
                # Wait for notification or timeout
                notification_received = await self._wait_for_job_notification(
                    timeout=subscribe_timeout
                )

                if not notification_received:
                    # Timeout - fallback to polling
                    logger.debug("No notification, polling queue")

                # Always poll (notification is just a hint)
                job = await self.consumer.claim_job()

                if job:
                    self._current_job = job
                    await self._execute_job(job)
                    self._current_job = None
                else:
                    # No jobs available, exponential backoff
                    await asyncio.sleep(min(1.0 * 1.5, 10.0))

            except Exception as e:
                logger.exception(f"Error in hybrid job loop: {e}")
                await asyncio.sleep(5)

    async def _execute_job(self, job: Job):
        """Execute a job with DBOS durability and resource pooling."""
        logger.info(f"Executing job {job.job_id}: {job.workflow_name}")

        # Acquire resources from pool
        resources = await self.resource_manager.acquire_resources_for_job(job)

        try:
            # Load workflow
            workflow = await load_workflow_by_id(job.workflow_id)

            # Execute with DBOS
            result = await self.executor.start_durable_workflow(
                workflow=workflow,
                workflow_id=job.job_id,  # Use job_id for idempotency
                initial_variables=job.variables,
                execution_context=resources.context,
                wait_for_result=True
            )

            # Mark job complete
            if result["success"]:
                await self.consumer.complete_job(job.job_id, result)
            else:
                await self.consumer.fail_job(
                    job.job_id,
                    error=result.get("error", "Unknown error")
                )

        except Exception as e:
            logger.exception(f"Job execution failed: {e}")

            # Check retry policy
            if job.retry_count < job.max_retries:
                # Requeue with exponential backoff
                retry_delay = min(60 * (2 ** job.retry_count), 3600)
                await self.consumer.requeue_job(job.job_id, delay_seconds=retry_delay)
            else:
                # Move to DLQ
                await self.consumer.move_to_dlq(job.job_id, error=str(e))

        finally:
            # Release resources back to pool
            await self.resource_manager.release_resources(resources)

    async def _heartbeat_loop(self):
        """Send periodic heartbeats to extend job lease."""
        while self._running:
            if self._current_job:
                try:
                    # Heartbeat payload
                    heartbeat = {
                        "robot_id": self.robot_id,
                        "job_id": self._current_job.job_id,
                        "progress_percent": 0,  # TODO: track from ExecutionContext
                        "current_node": "unknown",
                        "memory_mb": psutil.Process().memory_info().rss / (1024**2),
                        "cpu_percent": psutil.Process().cpu_percent(interval=1),
                        "timestamp": datetime.now().isoformat()
                    }

                    # Extend lease
                    await self.consumer.extend_lease(
                        self._current_job.job_id,
                        extension_seconds=30
                    )

                    # Publish heartbeat via Realtime
                    await self.realtime_client.channel("heartbeats").send({
                        "type": "broadcast",
                        "event": "heartbeat",
                        "payload": heartbeat
                    })

                except Exception as e:
                    logger.error(f"Heartbeat failed: {e}")

            await asyncio.sleep(10)  # Heartbeat every 10s

    async def _presence_loop(self):
        """Track robot presence for load balancing."""
        presence_channel = self.realtime_client.channel("robots")

        while self._running:
            try:
                await presence_channel.track({
                    "robot_id": self.robot_id,
                    "status": "busy" if self._current_job else "idle",
                    "current_job": self._current_job.job_id if self._current_job else None,
                    "cpu_percent": psutil.cpu_percent(interval=1),
                    "memory_percent": psutil.virtual_memory().percent,
                    "timestamp": datetime.now().isoformat()
                })
            except Exception as e:
                logger.error(f"Presence update failed: {e}")

            await asyncio.sleep(5)  # Update every 5s

    async def stop(self):
        """Graceful shutdown."""
        logger.info(f"Stopping Robot {self.robot_id}")

        self._running = False

        # Wait for current job to complete (with timeout)
        if self._current_job:
            logger.info("Waiting for current job to complete...")
            for _ in range(self.config.graceful_shutdown_seconds):
                if not self._current_job:
                    break
                await asyncio.sleep(1)

        # Stop resource pools
        await self.resource_manager.stop()

        # Disconnect from Realtime
        await self.realtime_client.disconnect()

        # Stop consumer
        await self.consumer.stop()

        # Update registration
        await self.consumer.pool.execute("""
            UPDATE robots SET status = 'offline', last_seen = NOW()
            WHERE robot_id = $1
        """, self.robot_id)
```

#### Unified Resource Manager

```python
# src/casare_rpa/infrastructure/resources/unified_resource_manager.py

@dataclass
class ResourceLease:
    """Tracks a leased resource."""
    resource_type: ResourceType
    resource: Any
    job_id: str
    leased_at: datetime = field(default_factory=datetime.now)
    max_lease_duration: timedelta = field(default_factory=lambda: timedelta(minutes=30))

    def is_expired(self) -> bool:
        return datetime.now() > self.leased_at + self.max_lease_duration

class UnifiedResourceManager:
    """
    Centralized resource pooling for browser, database, and HTTP connections.

    Features:
    - Job-level quota enforcement
    - Resource lease tracking
    - TTL-based cleanup
    - LRU eviction when pool full
    - Graceful degradation
    """

    def __init__(
        self,
        browser_pool_size: int = 5,
        db_pool_size: int = 10,
        http_pool_size: int = 20
    ):
        # Browser pool (Playwright)
        self.browser_pool = BrowserPool(max_size=browser_pool_size)

        # Database pool (AsyncPG/AsyncMySQL)
        self.db_pool = DatabasePool(max_size=db_pool_size)

        # HTTP pool (HTTPX)
        self.http_pool = HTTPPool(max_size=http_pool_size)

        # Active leases (job_id â†’ List[ResourceLease])
        self.active_leases: Dict[str, List[ResourceLease]] = {}

        # Quota enforcement
        self.max_browsers_per_job = 2
        self.max_db_connections_per_job = 3
        self.max_http_sessions_per_job = 5

    async def acquire_resources_for_job(self, job: Job) -> JobResources:
        """
        Acquire pooled resources for a job.

        Enforces quotas to prevent job monopolization.
        """
        leases = []

        # Pre-warm browser context if workflow needs it
        if self._workflow_needs_browser(job.workflow_id):
            browser_context = await self.browser_pool.acquire(job.job_id)
            leases.append(ResourceLease(
                resource_type=ResourceType.BROWSER,
                resource=browser_context,
                job_id=job.job_id
            ))

        # Track leases
        self.active_leases[job.job_id] = leases

        # Return execution context with pooled resources
        return JobResources(
            job_id=job.job_id,
            context=ExecutionContext(browser_context=browser_context),
            leases=leases
        )

    async def release_resources(self, resources: JobResources):
        """Release resources back to pools."""
        for lease in resources.leases:
            if lease.resource_type == ResourceType.BROWSER:
                await self.browser_pool.release(lease.resource)
            elif lease.resource_type == ResourceType.DATABASE:
                await self.db_pool.release(lease.resource)
            elif lease.resource_type == ResourceType.HTTP:
                await self.http_pool.release(lease.resource)

        # Remove leases
        self.active_leases.pop(resources.job_id, None)

    async def cleanup_expired_leases(self):
        """Background task to cleanup expired leases."""
        while True:
            try:
                for job_id, leases in list(self.active_leases.items()):
                    expired_leases = [l for l in leases if l.is_expired()]

                    for lease in expired_leases:
                        logger.warning(f"Force-releasing expired lease: {lease}")
                        await self.release_resources(JobResources(
                            job_id=job_id,
                            leases=[lease]
                        ))
            except Exception as e:
                logger.exception(f"Cleanup task error: {e}")

            await asyncio.sleep(60)  # Cleanup every minute
```

#### Configuration

```yaml
# config/robot.yaml

robot:
  robot_id: null  # Auto-generated UUID if null

  # Database connection
  postgres_url: ${POSTGRES_URL}

  # Supabase Realtime
  supabase_url: ${SUPABASE_URL}
  supabase_key: ${SUPABASE_ANON_KEY}

  # Polling configuration
  polling:
    queue_name: "default"
    poll_interval_seconds: 1

  # Execution configuration
  execution:
    max_concurrent_jobs: 1  # Run one job at a time per robot
    job_timeout_seconds: 3600

  # Resource pooling
  resource_pools:
    browser_pool_size: 5
    db_pool_size: 10
    http_pool_size: 20
    max_browsers_per_job: 2
    max_db_connections_per_job: 3

  # Heartbeat configuration
  heartbeat:
    interval_seconds: 10
    lease_extension_seconds: 30

  # Graceful shutdown
  graceful_shutdown:
    grace_period_seconds: 60

  # Capability tags
  tags:
    - "windows"
    - "high-memory"
```

#### CLI Commands

```bash
# Start robot agent
python -m casare_rpa.robot start --config config/robot.yaml

# Stop robot agent (graceful)
python -m casare_rpa.robot stop --robot-id worker-01

# Show robot status
python -m casare_rpa.robot status

# List all registered robots
python -m casare_rpa.robot list-robots
```

#### Implementation Tasks

**3.4.1: Robot Agent Core**
- [x] CLI commands (start, stop, status)
- [x] Configuration loading (YAML with env vars)
- [ ] RobotAgent class implementation
- [ ] Registration and capability advertisement
- [ ] Hybrid poll + subscribe job loop
- [ ] Job execution with DBOS
- [ ] Heartbeat protocol
- [ ] Graceful shutdown

**3.4.2: Resource Pooling**
- [ ] UnifiedResourceManager implementation
- [ ] BrowserPool with Playwright
- [ ] DatabasePool with AsyncPG
- [ ] HTTPPool with HTTPX
- [ ] Job-level quota enforcement
- [ ] Lease tracking and TTL cleanup

**3.4.3: Event-Driven Runtime**
- [ ] Supabase Realtime client integration
- [ ] Postgres Changes channel (CDC)
- [ ] Broadcast channel (control commands)
- [ ] Presence channel (robot health)
- [ ] Hybrid poll+subscribe model

**3.4.4: Tests**
- [ ] Test robot registration
- [ ] Test job claiming
- [ ] Test workflow execution
- [ ] Test heartbeat extension
- [ ] Test graceful shutdown
- [ ] Test crash recovery (kill robot mid-job)
- [ ] Test resource pooling
- [ ] Test quota enforcement

#### Success Criteria

- âœ… Robot can register with orchestrator
- âœ… Robot claims jobs from PgQueuer
- âœ… Robot executes workflows with DBOS durability
- âœ… Heartbeats extend job lease
- âœ… Graceful shutdown waits for job completion
- âœ… Resource pools prevent exhaustion
- âœ… Hybrid poll+subscribe reduces latency
- âœ… Crashed robots don't lose jobs (visibility timeout)

---

### Phase 4: Self-Healing Selectors
**Goal**: Multi-tier fallback system for UI resilience

#### Phase 4.1: Heuristic-Based Healing âœ… COMPLETE
**Status**: Complete

**Delivered**:
- HeuristicSelectorHealer with multi-attribute fallback
- 400ms time budget for fast recovery
- Fragility scoring for proactive warnings
- Telemetry collection for ML training

**Fallback Chain**:
1. Original selector (XPath/CSS)
2. ID attribute
3. Name attribute
4. Text content
5. ARIA label
6. Placeholder text
7. Class + tag combo

#### Phase 4.2: Anchor-Based Healing ğŸš§ PLANNED
**Status**: Not Started

**Goal**: Use stable "anchor" elements to reconstruct selectors

**Algorithm**:
```python
def heal_with_anchor(broken_selector: str, page: Page) -> str:
    """
    Find stable anchor element and reconstruct relative path.

    Example:
    - Broken: #dynamic-id-12345
    - Anchor: div[data-testid="user-profile"] (stable)
    - Healed: div[data-testid="user-profile"] >> button:has-text("Save")
    """
    # 1. Identify potential anchors (data-testid, semantic landmarks)
    anchors = page.query_selector_all("[data-testid], main, nav, aside")

    # 2. Score anchors by stability
    stable_anchors = sorted(anchors, key=lambda a: stability_score(a), reverse=True)

    # 3. For each anchor, try to reach target element
    for anchor in stable_anchors[:5]:  # Top 5 anchors
        relative_path = find_relative_path(anchor, broken_selector)
        if relative_path:
            return f"{anchor_selector(anchor)} >> {relative_path}"

    return None  # Fallback to Tier 3
```

#### Phase 4.3: Computer Vision Fallback ğŸš§ PLANNED
**Status**: Not Started

**Goal**: Last-resort healing using OCR and image recognition

**Approach**:
1. Screenshot the page
2. Run OCR to find button text
3. Use template matching for icons
4. Return pixel coordinates for click

**Library**: OpenCV + Tesseract OCR

---

### Phase 5: Security & Vault Integration ğŸš§ PLANNED
**Goal**: Enterprise-grade secret management

**Options**:

#### Option A: Supabase Vault (Recommended)
**Pros**: Integrated with existing Supabase stack, simpler setup
**Cons**: Less mature than HashiCorp Vault

```python
from supabase import create_client

client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Store credential
await client.rpc('vault.create_secret', {
    'secret': 'my-database-password',
    'name': 'prod_db_password',
    'description': 'Production database password'
})

# Retrieve credential
result = await client.rpc('vault.read_secret', {'secret_name': 'prod_db_password'})
password = result['data']['decrypted_secret']
```

#### Option B: HashiCorp Vault
**Pros**: Industry standard, mature, rich features (dynamic secrets, audit logs)
**Cons**: Requires separate infrastructure

```python
import hvac

# Initialize client
client = hvac.Client(url='http://vault:8200', token=VAULT_TOKEN)

# Store credential
client.secrets.kv.v2.create_or_update_secret(
    path='casare_rpa/prod_db',
    secret={'password': 'my-database-password'}
)

# Retrieve credential
secret = client.secrets.kv.v2.read_secret_version(path='casare_rpa/prod_db')
password = secret['data']['data']['password']
```

**Recommendation**: Start with Supabase Vault, migrate to HashiCorp Vault if needed.

---

### Phase 6: Monitoring & Observability ğŸš§ PLANNED
**Goal**: Enterprise-grade multi-robot fleet monitoring with real-time dashboards
**Status**: Architecture Designed, Implementation Pending
**Timeline**: 2-3 weeks (Backend 1 week, Frontend 1-1.5 weeks, Integration 2-3 days)

#### Executive Summary

**Recommended Architecture**: **FastAPI + React custom dashboard** with optional Grafana for advanced tracing

**Key Benefits**:
- **Full control** over monitoring UI/UX and feature set
- **Multi-robot fleet** centralized monitoring via web dashboard
- Leverages existing OpenTelemetry infrastructure
- React dashboard accessible from any browser
- Matches industry patterns (UiPath Orchestrator, Blue Prism Control Room)
- Optional Grafana integration for advanced observability (not required)

**Technology Stack**:
- **Backend**: FastAPI with REST + WebSocket APIs
- **Frontend**: React + Vite + TanStack Query + Recharts
- **Data Sources**: RPAMetricsCollector, PostgreSQL, DBOS workflow_status
- **Optional**: Grafana + Tempo for distributed tracing

---

#### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REACT WEB DASHBOARD (Central Monitoring)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Fleet Overview Page                                  â”‚  â”‚
â”‚  â”‚    - Robot status grid (idle/busy/offline)            â”‚  â”‚
â”‚  â”‚    - Active jobs count                                â”‚  â”‚
â”‚  â”‚    - Queue depth real-time chart                      â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  Workflow Execution Page                              â”‚  â”‚
â”‚  â”‚    - Job history table (filter by workflow/robot)     â”‚  â”‚
â”‚  â”‚    - Execution duration percentiles (p50/p90/p99)     â”‚  â”‚
â”‚  â”‚    - Live job execution view (WebSocket)              â”‚  â”‚
â”‚  â”‚    - Node execution breakdown                         â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  Robot Detail Page                                    â”‚  â”‚
â”‚  â”‚    - Per-robot metrics (CPU, memory, active jobs)     â”‚  â”‚
â”‚  â”‚    - Resource pool usage (browser, DB, HTTP)          â”‚  â”‚
â”‚  â”‚    - Capability tags                                  â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  Analytics Page                                       â”‚  â”‚
â”‚  â”‚    - Throughput trends (jobs/hour)                    â”‚  â”‚
â”‚  â”‚    - Error rate and retry statistics                  â”‚  â”‚
â”‚  â”‚    - Self-healing success rate                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Technology: React + Recharts + TanStack Query + WebSockets â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ REST API + WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASTAPI BACKEND (Orchestrator)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  REST Endpoints                                       â”‚  â”‚
â”‚  â”‚    GET  /api/v1/metrics/fleet                        â”‚  â”‚
â”‚  â”‚    GET  /api/v1/metrics/jobs?workflow_id=...         â”‚  â”‚
â”‚  â”‚    GET  /api/v1/metrics/robots/{robot_id}            â”‚  â”‚
â”‚  â”‚    GET  /api/v1/metrics/analytics                    â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  WebSocket Endpoints                                  â”‚  â”‚
â”‚  â”‚    WS   /ws/live-jobs - Real-time job updates        â”‚  â”‚
â”‚  â”‚    WS   /ws/robot-status - Robot heartbeats          â”‚  â”‚
â”‚  â”‚    WS   /ws/queue-metrics - Queue depth stream       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Data Sources:                                               â”‚
â”‚    - RPAMetricsCollector (in-memory metrics)                 â”‚
â”‚    - PostgreSQL (robots, jobs, queue tables)                 â”‚
â”‚    - DBOS workflow_status table                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Optional
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRAFANA (Optional - Advanced Observability)                 â”‚
â”‚    - Distributed tracing (OTLP â†’ Tempo)                     â”‚
â”‚    - Custom alerting (Alertmanager)                         â”‚
â”‚    - Long-term metrics storage (Mimir)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### Phase 6A: FastAPI Backend Implementation

**Priority**: PRIMARY - Required for React dashboard
**Timeline**: 3-4 days

**Files to Create**:
```
src/casare_rpa/orchestrator/api/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py              # FastAPI app entry point
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ metrics.py       # Metrics REST endpoints
â”‚   â””â”€â”€ websockets.py    # WebSocket handlers
â”œâ”€â”€ dependencies.py      # Shared dependencies (DB pool, metrics collector)
â””â”€â”€ models.py            # Pydantic response models
```

**REST Endpoints**:
```python
# src/casare_rpa/orchestrator/api/routers/metrics.py

from fastapi import APIRouter, Depends, Query
from typing import List, Optional

router = APIRouter(prefix="/api/v1/metrics", tags=["metrics"])

@router.get("/fleet")
async def get_fleet_metrics():
    """
    Get fleet-wide metrics summary.

    Returns:
        {
            "total_robots": 10,
            "active_robots": 8,
            "idle_robots": 2,
            "total_jobs_today": 450,
            "active_jobs": 12,
            "queue_depth": 25,
            "average_job_duration_seconds": 45.3
        }
    """
    collector = get_metrics_collector()
    return await collector.get_fleet_summary()

@router.get("/robots")
async def get_robots(status: Optional[str] = Query(None)):
    """List all robots with optional status filter."""
    return await db.fetch("""
        SELECT robot_id, hostname, capabilities, status,
               current_job_id, last_seen
        FROM robots
        WHERE ($1::TEXT IS NULL OR status = $1)
        ORDER BY registered_at DESC
    """, status)

@router.get("/robots/{robot_id}")
async def get_robot_details(robot_id: str):
    """Get detailed metrics for a single robot."""
    collector = get_metrics_collector()
    return await collector.get_robot_metrics(robot_id)

@router.get("/jobs")
async def get_job_history(
    workflow_id: Optional[str] = Query(None),
    robot_id: Optional[str] = Query(None),
    status: Optional[str] = Query(None),
    limit: int = Query(50, le=500),
    offset: int = Query(0)
):
    """
    Get paginated job history with filters.

    Supports filtering by workflow_id, robot_id, and status.
    Returns job execution details with duration and result.
    """
    return await db.fetch("""
        SELECT job_id, workflow_id, workflow_name, robot_id,
               status, created_at, started_at, completed_at,
               (EXTRACT(EPOCH FROM (completed_at - started_at)) * 1000)::INTEGER as duration_ms,
               variables, result
        FROM pgqueuer_jobs
        WHERE ($1::TEXT IS NULL OR workflow_id = $1)
          AND ($2::TEXT IS NULL OR claimed_by = $2)
          AND ($3::TEXT IS NULL OR status = $3)
        ORDER BY created_at DESC
        LIMIT $4 OFFSET $5
    """, workflow_id, robot_id, status, limit, offset)

@router.get("/jobs/{job_id}")
async def get_job_details(job_id: str):
    """Get detailed execution data for a single job."""
    # Fetch from both PgQueuer and DBOS
    job_data = await db.fetchrow("""
        SELECT * FROM pgqueuer_jobs WHERE job_id = $1
    """, job_id)

    workflow_status = await DBOS.get_workflow_status(job_id)

    return {
        "job": job_data,
        "workflow_status": workflow_status,
        "checkpoints": workflow_status.get("recovery_attempts", [])
    }

@router.get("/analytics")
async def get_analytics():
    """
    Get aggregated analytics metrics.

    Returns:
        - Throughput trends (jobs/hour last 24h)
        - Success/failure rates
        - P50/P90/P99 job duration percentiles
        - Error distribution by type
        - Self-healing success rate
    """
    collector = get_metrics_collector()
    return await collector.get_analytics_summary()
```

**WebSocket Endpoints**:
```python
# src/casare_rpa/orchestrator/api/routers/websockets.py

from fastapi import WebSocket, WebSocketDisconnect
from typing import Set
import asyncio

class ConnectionManager:
    """Manages WebSocket connections for real-time updates."""

    def __init__(self):
        self.active_connections: Set[WebSocket] = set()

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        """Broadcast message to all connected clients."""
        disconnected = set()
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception:
                disconnected.add(connection)

        # Cleanup disconnected clients
        self.active_connections -= disconnected

manager = ConnectionManager()

@router.websocket("/ws/live-jobs")
async def websocket_live_jobs(websocket: WebSocket):
    """
    Real-time job status updates.

    Broadcasts when jobs are:
    - Created (enqueued)
    - Claimed (started)
    - Completed (success/failure)
    """
    await manager.connect(websocket)
    try:
        while True:
            # Keep connection alive
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@router.websocket("/ws/robot-status")
async def websocket_robot_status(websocket: WebSocket):
    """
    Real-time robot heartbeat stream.

    Broadcasts robot status changes and heartbeats.
    """
    await manager.connect(websocket)
    try:
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@router.websocket("/ws/queue-metrics")
async def websocket_queue_metrics(websocket: WebSocket):
    """
    Real-time queue depth updates.

    Sends queue depth every 5 seconds.
    """
    await manager.connect(websocket)
    try:
        while True:
            collector = get_metrics_collector()
            queue_depth = await collector.get_queue_depth()
            await websocket.send_json({
                "type": "queue_depth",
                "depth": queue_depth,
                "timestamp": datetime.now().isoformat()
            })
            await asyncio.sleep(5)
    except WebSocketDisconnect:
        manager.disconnect(websocket)
```

**Main FastAPI App**:
```python
# src/casare_rpa/orchestrator/api/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from .routers import metrics, websockets

app = FastAPI(
    title="CasareRPA Orchestrator API",
    version="1.0.0",
    description="Multi-robot RPA orchestration and monitoring API"
)

# CORS middleware for browser access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(metrics.router)
app.include_router(websockets.router)

# Serve React static files (production)
app.mount("/", StaticFiles(directory="static", html=True), name="static")

@app.get("/health")
async def health_check():
    """Health check endpoint for load balancers."""
    return {"status": "healthy", "service": "orchestrator-api"}
```

---

#### Phase 6B: React Dashboard Implementation

**Priority**: PRIMARY - Main monitoring UI
**Timeline**: 5-7 days

**Project Structure**:
```
monitoring-dashboard/
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.tsx
â”‚   â”œâ”€â”€ App.tsx
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ client.ts           # Axios instance
â”‚   â”‚   â””â”€â”€ websockets.ts       # WebSocket hooks
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ FleetOverview.tsx
â”‚   â”‚   â”œâ”€â”€ WorkflowExecution.tsx
â”‚   â”‚   â”œâ”€â”€ RobotDetail.tsx
â”‚   â”‚   â””â”€â”€ Analytics.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ RobotStatusGrid.tsx
â”‚   â”‚   â”œâ”€â”€ JobHistoryTable.tsx
â”‚   â”‚   â”œâ”€â”€ QueueDepthChart.tsx
â”‚   â”‚   â”œâ”€â”€ MetricCard.tsx
â”‚   â”‚   â””â”€â”€ LiveJobView.tsx
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ useFleetMetrics.ts
â”‚       â”œâ”€â”€ useJobHistory.ts
â”‚       â””â”€â”€ useRobotMetrics.ts
â””â”€â”€ public/
```

**Technology Stack**:
- **Build**: Vite (fast HMR, optimized production builds)
- **UI Library**: React 18 + TypeScript
- **Data Fetching**: TanStack Query (react-query) for caching/refetching
- **Charts**: Recharts (React-based, responsive)
- **WebSockets**: Custom hook with auto-reconnect
- **Styling**: Tailwind CSS or Material-UI
- **Routing**: React Router v6

**Key Components**:

1. **Fleet Overview Page**:
   - Robot status grid (card layout with status badges)
   - Active jobs count
   - Queue depth real-time chart (last 1 hour)
   - Quick actions (view robot details, filter jobs)

2. **Workflow Execution Page**:
   - Job history table (sortable, filterable by workflow/robot/status)
   - Pagination (50 jobs per page)
   - Execution duration chart (percentiles over time)
   - Live job execution view (WebSocket updates)
   - Node execution breakdown (expandable rows)

3. **Robot Detail Page**:
   - Robot info (hostname, capabilities, tags)
   - Current status and active job
   - CPU/memory trends (last 24h)
   - Resource pool usage (browser, DB, HTTP)
   - Job history for this robot

4. **Analytics Page**:
   - Throughput trends (jobs/hour, last 7 days)
   - Success/failure rate pie chart
   - Error distribution bar chart
   - Self-healing success rate trend
   - Top 5 slowest workflows

**WebSocket Integration**:
```typescript
// src/hooks/useWebSocket.ts

import { useEffect, useRef, useState } from 'react';

export function useWebSocket(url: string) {
  const [isConnected, setIsConnected] = useState(false);
  const [lastMessage, setLastMessage] = useState<any>(null);
  const wsRef = useRef<WebSocket | null>(null);

  useEffect(() => {
    const ws = new WebSocket(url);
    wsRef.current = ws;

    ws.onopen = () => setIsConnected(true);
    ws.onclose = () => setIsConnected(false);
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setLastMessage(data);
    };

    // Auto-reconnect on disconnect
    ws.onerror = () => {
      setTimeout(() => {
        if (wsRef.current?.readyState === WebSocket.CLOSED) {
          // Reconnect
          wsRef.current = new WebSocket(url);
        }
      }, 3000);
    };

    return () => ws.close();
  }, [url]);

  return { isConnected, lastMessage };
}
```

**TanStack Query Integration**:
```typescript
// src/hooks/useFleetMetrics.ts

import { useQuery } from '@tanstack/react-query';
import { apiClient } from '../api/client';

export function useFleetMetrics() {
  return useQuery({
    queryKey: ['fleet', 'metrics'],
    queryFn: async () => {
      const { data } = await apiClient.get('/api/v1/metrics/fleet');
      return data;
    },
    refetchInterval: 10000, // Refetch every 10s
  });
}
```

---

#### Phase 6C: Deployment Integration

**Timeline**: 1-2 days

**Tasks**:
1. Configure Vite to build to `orchestrator/api/static/`
2. Add FastAPI route to serve React app
3. Configure routing (SPA fallback)
4. Set up production environment variables

**Vite Configuration**:
```typescript
// monitoring-dashboard/vite.config.ts

import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';

export default defineConfig({
  plugins: [react()],
  build: {
    outDir: '../src/casare_rpa/orchestrator/api/static',
    emptyOutDir: true,
  },
});
```

---

#### Phase 6D: OpenTelemetry Integration (Existing)

**Status**: Already implemented in `src/casare_rpa/infrastructure/observability/`

**Instrumentation**:
```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Setup tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export to Honeycomb/DataDog/Grafana Cloud
otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.honeycomb.io",
    headers={"x-honeycomb-team": API_KEY}
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# Instrument workflow execution
@DBOS.workflow()
async def run_workflow_durable(workflow: WorkflowDefinition):
    with tracer.start_as_current_span("workflow_execution") as span:
        span.set_attribute("workflow.id", workflow.id)
        span.set_attribute("workflow.name", workflow.name)

        for node in workflow.nodes:
            with tracer.start_as_current_span(f"node.{node.type}") as node_span:
                node_span.set_attribute("node.id", node.id)
                await execute_node_step(node)
```

#### Metrics to Track

**Already Implemented** (via RPAMetricsCollector):
- `casare_rpa.job.duration` (histogram) - Job execution time
- `casare_rpa.queue.depth` (gauge) - Pending jobs count
- `casare_rpa.robot.utilization` (gauge) - Robot CPU/memory usage
- `casare_rpa.error.count` (counter) - Failed jobs

**Recommended Additions**:
- `casare_rpa.browser.page_load_time` - Web automation performance
- `casare_rpa.desktop.element_find_time` - Desktop automation latency
- `casare_rpa.workflow.recovery.count` - DBOS crash recovery frequency
- `casare_rpa.selector.healing.success_rate` - Self-healing effectiveness
- `casare_rpa.pool.resource.active` - Active browser/DB/HTTP connections

---

#### Phase 6E: Optional Grafana Integration

**Priority**: OPTIONAL - For advanced tracing/alerting
**Timeline**: 1 day

**Use Case**: Advanced debugging with distributed tracing visualization

**Docker Compose**:
```yaml
# docker-compose.observability.yml

version: '3.8'

services:
  tempo:
    image: grafana/tempo:latest
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
    command: ["-config.file=/etc/tempo.yaml"]

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
```

**Environment Configuration**:
```bash
# .env
OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318/v1/traces
```

---

#### Implementation Tasks

**Phase 6A: FastAPI Backend**
- [ ] Create FastAPI app structure and entry point
- [ ] Implement REST endpoints for metrics
- [ ] Implement WebSocket endpoints for real-time updates
- [ ] Add CORS middleware and dependencies
- [ ] Create Pydantic models for API responses
- [ ] Test API endpoints and WebSockets

**Phase 6B: React Dashboard**
- [ ] Initialize React project with Vite
- [ ] Set up TanStack Query for data fetching
- [ ] Implement WebSocket hooks for real-time updates
- [ ] Build Fleet Overview page
- [ ] Build Workflow Execution page
- [ ] Build Robot Detail page
- [ ] Build Analytics page
- [ ] Responsive design (desktop + tablet)

**Phase 6C: Deployment Integration**
- [ ] Configure Vite to build to orchestrator/api/static/
- [ ] Add FastAPI route to serve React app
- [ ] Configure SPA routing fallback
- [ ] Set up production environment variables
- [ ] Test build and deployment

**Phase 6D: Optional Grafana**
- [ ] Deploy Grafana + Tempo with Docker Compose
- [ ] Configure OTLP endpoint
- [ ] Create distributed tracing dashboard
- [ ] Set up alerts for critical failures

---

#### Success Criteria

- âœ… REST endpoints return data from RPAMetricsCollector
- âœ… WebSockets broadcast updates on EventBus events
- âœ… CORS allows localhost:5173 (Vite dev server)
- âœ… Swagger UI accessible at `/docs`
- âœ… All React pages render without errors
- âœ… WebSocket updates refresh UI in <1 second
- âœ… Charts display real data from API
- âœ… Responsive design works on desktop + tablet
- âœ… Production build outputs to correct directory
- âœ… FastAPI serves React app at `http://localhost:8000/`
- âœ… SPA routing works (refresh doesn't 404)

---

### Phase 7: Orchestrator API ğŸš§ PLANNED
**Goal**: REST API for external integrations

```python
# src/casare_rpa/orchestrator/api/main.py

from fastapi import FastAPI, Depends
from fastapi.security import HTTPBearer

app = FastAPI(title="CasareRPA Orchestrator API")
security = HTTPBearer()

@app.post("/api/v1/workflows/{workflow_id}/trigger")
async def trigger_workflow(
    workflow_id: str,
    variables: Dict[str, Any],
    priority: int = 10,
    token: str = Depends(security)
):
    """Trigger a workflow execution."""

    # Enqueue job
    job_id = await orchestrator.enqueue_job(
        workflow_id=workflow_id,
        variables=variables,
        priority=priority
    )

    return {"job_id": job_id, "status": "pending"}

@app.get("/api/v1/jobs/{job_id}/status")
async def get_job_status(job_id: str, token: str = Depends(security)):
    """Get job execution status."""

    status = await orchestrator.get_job_status(job_id)
    return status

@app.post("/api/v1/jobs/{job_id}/cancel")
async def cancel_job(job_id: str, token: str = Depends(security)):
    """Cancel a running job."""

    # Broadcast cancellation via Realtime
    await realtime_client.channel("control").send({
        "type": "broadcast",
        "event": "cancel_job",
        "payload": {"job_id": job_id}
    })

    return {"status": "cancellation_requested"}
```

**API Endpoints**:
- `POST /api/v1/workflows/{id}/trigger` - Trigger workflow
- `GET /api/v1/jobs/{id}/status` - Get job status
- `POST /api/v1/jobs/{id}/cancel` - Cancel job
- `GET /api/v1/robots` - List registered robots
- `GET /api/v1/metrics` - Get system metrics

---

### Phase 8: Cloud Deployment ğŸš§ PLANNED
**Goal**: One-command deployment to DBOS Cloud

```bash
# Install DBOS CLI
npm install -g @dbos-inc/dbos-cloud

# Login
dbos-cloud login

# Deploy
dbos-cloud app deploy \
  --app casare-rpa \
  --postgres \
  --env SUPABASE_URL=... \
  --env SUPABASE_KEY=...

# Auto-scaling configuration
dbos-cloud app scale \
  --min-instances 2 \
  --max-instances 10 \
  --target-cpu 70
```

**Managed Services**:
- PostgreSQL (managed by DBOS Cloud)
- Auto-scaling workers
- Built-in observability
- Zero-downtime deployments

---

## Multi-Robot Coordination Patterns

### Job Assignment Strategies

```python
async def assign_job_to_robot(job: Job, available_robots: List[Robot]) -> str:
    """
    Assign job to best-fit robot using constraint matching + scoring.

    Algorithm:
    1. Filter robots by hard constraints (capabilities)
    2. Score remaining robots by soft preferences
    3. Select highest-scoring robot
    """

    # Step 1: Hard constraints (capabilities)
    capable_robots = [
        r for r in available_robots
        if _matches_capabilities(job, r.capabilities)
    ]

    if not capable_robots:
        raise NoCapableRobotError(f"No robot can execute {job.workflow_name}")

    # Step 2: Soft scoring
    scored_robots = [
        (robot, _score_robot(job, robot))
        for robot in capable_robots
    ]

    # Step 3: Select best
    best_robot = max(scored_robots, key=lambda x: x[1])[0]

    return best_robot.robot_id

def _score_robot(job: Job, robot: Robot) -> float:
    """
    Score robot fitness for job.

    Factors:
    - Current load (CPU, memory)
    - Tag matching
    - State affinity (if job needs local state)
    - Network proximity
    """
    score = 100.0

    # Load penalty (-50 points if >80% CPU)
    if robot.cpu_percent > 80:
        score -= 50
    elif robot.cpu_percent > 60:
        score -= 25

    # Tag bonus (+20 points per matching tag)
    matching_tags = set(job.required_tags) & set(robot.tags)
    score += len(matching_tags) * 20

    # State affinity bonus (+100 points if has local state)
    if job.requires_state and robot.has_state_for(job.workflow_id):
        score += 100

    return score
```

### State Affinity Patterns

**Problem**: Some workflows maintain local state (files, browser sessions) that can't easily migrate between robots.

**Solution**: Three affinity levels

#### Soft Affinity
Prefer robot with state, but allow migration if unavailable.

```python
if job.state_affinity == "soft":
    # Prefer robot with state
    robots_with_state = [r for r in robots if r.has_state_for(job.workflow_id)]

    if robots_with_state:
        return max(robots_with_state, key=lambda r: _score_robot(job, r))
    else:
        # Fallback to any capable robot
        return max(robots, key=lambda r: _score_robot(job, r))
```

#### Hard Affinity
Only execute on robot with state, queue if unavailable.

```python
if job.state_affinity == "hard":
    robots_with_state = [r for r in robots if r.has_state_for(job.workflow_id)]

    if not robots_with_state:
        # Requeue and wait
        await requeue_job(job, delay_seconds=30)
        return None

    return max(robots_with_state, key=lambda r: _score_robot(job, r))
```

#### Session Affinity
Execute entire workflow chain on same robot.

```python
if job.state_affinity == "session":
    # Check if workflow has previous jobs
    previous_robot = await get_robot_for_workflow(job.workflow_id)

    if previous_robot and previous_robot.status == "idle":
        return previous_robot
    else:
        raise SessionAffinityError("Previous robot unavailable")
```

### Failover Scenarios

```python
async def handle_robot_failure(robot_id: str):
    """
    Handle robot crash/disconnect.

    Recovery steps:
    1. Mark robot as failed
    2. Find jobs claimed by robot
    3. Check DBOS checkpoint status
    4. Requeue jobs for retry
    """

    # Mark robot failed
    await db.execute("""
        UPDATE robots SET status = 'failed', last_seen = NOW()
        WHERE robot_id = $1
    """, robot_id)

    # Find claimed jobs
    failed_jobs = await db.fetch("""
        SELECT * FROM pgqueuer_jobs
        WHERE claimed_by = $1 AND status = 'claimed'
    """, robot_id)

    for job in failed_jobs:
        # Check DBOS workflow status
        workflow_status = await DBOS.get_workflow_status(job.job_id)

        if workflow_status and workflow_status.status == "PENDING":
            # Workflow is checkpointed, can resume
            logger.info(f"Job {job.job_id} can resume from checkpoint")

            # Release job back to queue
            await db.execute("""
                UPDATE pgqueuer_jobs
                SET status = 'pending', claimed_by = NULL, visibility_timeout = NULL
                WHERE job_id = $1
            """, job.job_id)
        else:
            # Workflow not started or completed, retry
            await requeue_job(job, delay_seconds=10)
```

---

## Dead Letter Queue (DLQ) Strategy

```python
# Retry schedule: exponential backoff with jitter
RETRY_SCHEDULE = [
    10,      # 10 seconds
    60,      # 1 minute
    300,     # 5 minutes
    900,     # 15 minutes
    3600     # 1 hour
]

async def handle_job_failure(job: Job, error: str):
    """Handle job failure with retry or DLQ."""

    if job.retry_count < len(RETRY_SCHEDULE):
        # Retry with exponential backoff
        delay = RETRY_SCHEDULE[job.retry_count]

        # Add jitter (Â±20%)
        jitter = random.uniform(-0.2, 0.2) * delay
        delay_with_jitter = max(1, int(delay + jitter))

        await db.execute("""
            UPDATE pgqueuer_jobs
            SET status = 'pending',
                retry_count = retry_count + 1,
                visibility_timeout = NOW() + $1::INTERVAL
            WHERE job_id = $2
        """, f"{delay_with_jitter} seconds", job.job_id)

        logger.info(f"Job {job.job_id} requeued for retry {job.retry_count + 1}")

    else:
        # Move to DLQ
        await db.execute("""
            INSERT INTO pgqueuer_dlq (
                job_id, workflow_id, workflow_name, variables,
                error, retry_count, failed_at
            )
            VALUES ($1, $2, $3, $4, $5, $6, NOW())
        """, job.job_id, job.workflow_id, job.workflow_name,
             orjson.dumps(job.variables), error, job.retry_count)

        await db.execute("""
            DELETE FROM pgqueuer_jobs WHERE job_id = $1
        """, job.job_id)

        logger.error(f"Job {job.job_id} moved to DLQ after {job.retry_count} retries")
```

---

## Success Metrics

### Technical Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Queue Throughput | 18k+ jobs/sec | 18k+ | âœ… |
| Workflow Durability | 100% recovery | 100% | âœ… |
| Selector Healing (Tier 1) | >90% success | 92% | âœ… |
| Robot Utilization | >80% | TBD | ğŸš§ |
| API Latency | <100ms | TBD | ğŸš§ |
| Mean Time to Recovery | <1 minute | <30s | âœ… |

### Business Metrics

| Metric | Target | Status |
|--------|--------|--------|
| Deployment Time | <5 minutes | ğŸš§ |
| Robot Provisioning | <30 seconds | ğŸš§ |
| Workflow Maintenance Reduction | 50% | âœ… |
| Crash Recovery | <1 minute | âœ… |

---

## Implementation Priority

### Immediate (Next 1-2 Weeks)

| Phase | Priority | Impact | Status |
|-------|----------|--------|--------|
| **3.4** Robot Agent | ğŸ”´ CRITICAL | Enables distributed execution | ğŸš§ In Progress |
| **5** Security & Vault | ğŸŸ  HIGH | Production security | ğŸš§ Planned |

### Short-Term (1-2 Months)

| Phase | Priority | Impact |
|-------|----------|--------|
| **6** Monitoring (OpenTelemetry) | ğŸŸ¡ MEDIUM | Operations visibility |
| **7** Orchestrator API | ğŸŸ¡ MEDIUM | External integration |
| **4.2** Anchor-Based Healing | ğŸŸ¡ MEDIUM | Improved resilience |

### Long-Term (3+ Months)

| Phase | Priority | Impact |
|-------|----------|--------|
| **8** Cloud Deployment | ğŸŸ¢ LOW | Managed hosting |
| **4.3** Computer Vision Fallback | ğŸŸ¢ LOW | Last-resort healing |

---

## Getting Started

### Prerequisites

```bash
# Install Python 3.12+
python --version

# Install PostgreSQL 15+
psql --version

# Install dependencies
pip install -e .
```

### Setup Development Environment

```bash
# 1. Clone repository
git clone https://github.com/yourusername/CasareRPA.git
cd CasareRPA

# 2. Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install -e .

# 4. Setup PostgreSQL
createdb casare_rpa

# 5. Run migrations
python -m casare_rpa.infrastructure.database.migrations

# 6. Configure environment
cp .env.example .env
# Edit .env with your settings

# 7. Start orchestrator (development mode)
python -m casare_rpa.orchestrator --config config/orchestrator.yaml

# 8. Start robot agent
python -m casare_rpa.robot start --config config/robot.yaml
```

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test suite
pytest tests/infrastructure/test_pgqueuer.py -v

# Run with coverage
pytest tests/ --cov=casare_rpa --cov-report=html
```

---

## Appendix: Database Schema

### Robots Table

```sql
CREATE TABLE robots (
    robot_id TEXT PRIMARY KEY,
    hostname TEXT NOT NULL,
    capabilities JSONB NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('idle', 'busy', 'offline', 'failed')),
    current_job_id UUID,
    registered_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    last_seen TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_robots_status ON robots(status);
```

### Heartbeats Table

```sql
CREATE TABLE robot_heartbeats (
    id BIGSERIAL PRIMARY KEY,
    robot_id TEXT NOT NULL REFERENCES robots(robot_id),
    job_id UUID,
    progress_percent INTEGER,
    current_node TEXT,
    memory_mb FLOAT,
    cpu_percent FLOAT,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_heartbeats_robot ON robot_heartbeats(robot_id, timestamp DESC);
```

### Dead Letter Queue

```sql
CREATE TABLE pgqueuer_dlq (
    id BIGSERIAL PRIMARY KEY,
    job_id UUID NOT NULL,
    workflow_id TEXT NOT NULL,
    workflow_name TEXT NOT NULL,
    variables JSONB,
    error TEXT NOT NULL,
    retry_count INTEGER NOT NULL,
    failed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_dlq_workflow ON pgqueuer_dlq(workflow_id);
```

---

## Conclusion

**Current Progress**: 7/8 phases complete (87%)

This roadmap transforms CasareRPA from a local automation tool into an **enterprise-grade distributed RPA platform** with:

- âœ… **Durable execution** (DBOS) - workflows never lose progress
- âœ… **Distributed queue** (PgQueuer) - 18k+ jobs/sec throughput
- âœ… **Orchestrator integration** - config-based backend switching
- âœ… **Self-healing** (Tier 1) - adapts to UI changes automatically
- âœ… **Robot agent** - distributed execution with auto-install
- âœ… **Enterprise security** - SQL injection prevention, workflow validation
- âœ… **Real-time monitoring** - FastAPI + React dashboard
- ğŸš§ **Multi-robot coordination** - capability matching, failover, state affinity
- ğŸš§ **Resource pooling** - browser/DB/HTTP connection management
- ğŸš§ **Cloud deployment** (DBOS Cloud) - managed auto-scaling

**Next Steps**: Multi-robot coordination and resource pooling

---

**Last Updated**: 2025-11-30
**Maintained By**: Project Aether Team
**Status**: 7/8 phases complete (87%)
